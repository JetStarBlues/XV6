---------------------------------------------------------------------------------------------
13) Address Spaces
---------------------------------------------------------------------------------------------

Multiprogramming

	. For example,

		------------------
		operating system
		(code, data, etc)
		------------------
		free
		------------------
		process_1
		(code, data, etc)
		------------------
		free
		------------------
		process_2
		(code, data, etc)
		------------------
		process_3
		(code, data, etc)
		------------------
		free
		------------------
		free
		------------------


Address space

	. The running program's view of memory
	. Includes
		. instructions
		. stack
		. heap - dynamically allocated, user-managed memory

	. For example,

		-------------
		program code
		-------------
		heap           // malloc'd data
		     |
		     v
		-------------
		free
		-------------
		     ^
		     |
		stack          // local variables, args to routines, return values etc.
		-------------

	. An abstraction.
	. The program isn't really at physical address 0


---------------------------------------------------------------------------------------------
14) Memory API
---------------------------------------------------------------------------------------------

Types of Memory

	~~~ Stack ~~~
	
	. allocations and deallocations managed implicitly
	. For example, to create space for an integer 'x' in the stack:

		void fx ()
		{
			int x;

			...
		}

		. The compiler makes sure you have space on the stack
		  when you call 'fx'
		. When you return from the function, the compiler deallocates
		  the memory


	~~~ Heap ~~~

	. allocations and deallocations managed explicity by program
	. For example, to create space for an integer 'x' in the stack:

		void fx ()
		{
			int* x;                                 // stack allocation; compiler makes room for an integer pointer

			x = ( int * ) malloc( sizeof( int ) );  // heap allocation; malloc makes room for an int in the heap

			...
		}


Malloc

	. Returns a pointer to type 'void'. This lets the programmer
	  decide what to do with the address


Free

	. Takes the pointer returned by malloc as only argument
	. Since the size of the allocated region is not passed in, this
	  means it must be tracked by the memory-allcoation library itself

		void fx ()
		{
			int* x;

			x = ( int * ) malloc( sizeof( int ) );

			...

			free( x );
		}


Underlying OS Support

	. malloc and free use system calls to ask the OS for more
	  memory (...) or to release memory (...)

	. 'brk' system call is used to change the location of the
	  end of the program's heap
	. It takes one argument - the address of the new heap end
	. It either increases or decreases the size of the heap based
	  on whether the new end is larger or smaller than the current

	. 'sbrk' system call is like 'brk' but instead of taking an
	  absolute address as an argument, it takes an increment/delta value
	. calling 'sbrk' with an increment of 0 can be used to find the
	  current location of the heap end
	. see man

	. 'mmap' is used to obtain memory from the OS
	. With the correct arugments, mmap can create an *anonymous*
	  memory region... ??
	. This memory can then be treated like a heap


Other Calls

	. 'calloc'  - like malloc but zeros regiion before returning

	. 'realloc'
		. Makes a new region of memory, copies the old region
		  into it, and returns a pointer to the new region.
		. If the new size is larger, has effect of growing
		  allocated size. If smaller, of shrinking.


---------------------------------------------------------------------------------------------
15) Address Translation
---------------------------------------------------------------------------------------------

Intro

	. hardware-based address translation


Dynamic (hardware-based) relocation

	. aka "base and bounds"
	. First incarnation of hardware-based address translation

	. Uses two hardware registers: 'base' and 'bound'

	. Assumes many contiguous blocks of size address-space are
	  available in memory: one per concurrent process wish to run...


	~~~ base ~~~

	. Each program is written and compiled as if it is loaded at
	  address zero
	. When the program starts running, the OS decides where in
	  physical memory it should be loaded and sets the 'base' register
	  to that value
	. Now when any memory reference is generated by the process,
	  it is translated by the CPU as follows:

		physicalAddress = virtualAddress + baseRegister

	. For example, when retrieving an instruction:

		physicalAddress = programCounter + baseRegister

	. "Dynamic" because translation happens at runtime


	~~~ bounds ~~~

	. The 'bounds' register helps with protection
	. Checks that the address is withing bounds

	. Can be defined in one of two ways:

		. Holds size of the address space, and thus checked against
		  virtual address

			boundsRegister = 16K  // size of address space

			if ( virtualAddress > boundsRegister ) { error }


		. Holds physical address of the end of the address space,
		  and thus checked against physical address

			boundsRegister = baseRegister + 16K

			if ( physicalAddress > boundsRegister ) { error }


	~~~ ... ~~~

	. The CPU should provide instructions to modify the base and
	  bounds registers, which the OS can use

	. The CPU must be able to generate *exceptions* when a user
	  program ties to access memory that is "out of bounds"
	. In this case, the CPU should stop executing the program
	  and arrange for the OS's "out of bounds" exception handler
	  to run - (i.e. trap)


OS Perspective

	~~~ first ... ~~~

	. When a process is created, find space for its address space
	  in memory
	. If we assume that:
		. each address space is smaller than the size of physical memory
		. all adress spaces are the same size
	. then we can view physical memory as an array of slots, and
	  track whether each one is free or in use
	. The simplest data structure the OS can use for this is a list
	  (aka "free list")

	. Things get more complicated when address spaces can vary in size


	~~~ second ... ~~~

	. When a process is terminated, reclaim all of its memory
	. I.e. put the process's memory back on the free list


	~~~ third ... ~~~

	. Since there is only one base and bounds register pair in a CPU,
	  when a context switch occurs, the OS must save and restore their
	  values

	. Specifically, when the OS decides to stop running a process,
	  it must save the values of the registers in some per-process
	  structure (ex. process control block)
	. And restore them when it resumes running the process


	~~~ fourth ... ~~~

	. Provide associaited exception handlers:
		. "out of bounds" access


	~~~ ... ~~~

	. An example interaction
	. Notice address translations are handled by the CPU with
	  no OS intervention
	. The OS intervenes only to handle traps/interrupts

		@boot

			OS
				. initialize trap table
				. start interrupt timer
				. initialize process table
				. initialize free list

		@run

			OS
				. To start process_1:
					. allocate entry in process table
					. allocate memory for process
					. set base/bound registers
					. return from trap (into process_1)...

			CPU
				. restore registers of process_1
				. switch to user mode
				. jump to process_1's (initial) PC


			User program
				. fetch instruction

			CPU
				. translate virtual address and perform fetch

			User program
				. execute instruction

			CPU
				. On timer interrupt:
					. switch to kernel mode
					. jump to interrupt handler

			OS
				. handle the trap
					. call 'swtch'
						. save registers to proc-struct(1) including base/bound
						. retore registers from proc-struct(2) including base/bound
				. return from trap (into process_2)...

			CPU
				. switch to user mode
				. jump to process_2's PC

			User program
				. process_2 runs until executes bad load

			CPU
				. switch to kernel mode
				. jump to trap handler

			OS
				. handle the trap
					. terminate process_2
					. deallocate process_2's memory
					. free process_2's entry in the process table


Summary

	. Fixed-size slots means potential waste of space when stack and/or
	  heap is small


---------------------------------------------------------------------------------------------
16) Segmentation
---------------------------------------------------------------------------------------------

Generalized base/bounds

	. Instead of having one base/bound pair, have one per logical
	  segment of the address spaces
		. code
		. heap
		. stack
	. Thus OS can place the segments in different parts of physical
	  memory, and avoid having large unused sections in-between...
	  (ex. between stack end and heap end...) ...

	. For hardware support, the CPU needs to have nSegments
	  base/bound register pairs

	. For example:

		address space

			------  0
			code
			------  2K
			free
			------  4K
			heap
			  |
			  v
			------
			free
			------
			  ^
			  |
			stack
			------  16K


		physical memory

			------  0
			OS
			------
			free
			------
			  ^
			  |
			stack
			------  ?
			free
			------  ?
			code
			------  ?
			heap
			  |
			  v
			------
			free
			------  64K


		segment  base  size
		-------  ----  ----
		code     32K   2K
		heap     34K   2K
		stack    28K   2K

		. virtual address 100 in code segment  -> 100 - codeBase + 32K
		                                        = 100 - 0 + 32K
		                                        = ~ 32100     (or exactly 32868)

		. virtual address 5200 in heap segment -> 5200 - heapBase + 34K
		                                        = 5200 - 4096 + 34K
		                                        = ~ 35200     (or exactly 35920)


Which segment are we referring to?

	. How does the CPU know which segment an address refers to and
	  which offset to use when translating?

	. One approach is to chop up the address space into segments
	  based on the top few bits of the virtual address... ??

	. For example, in our 14-bit virtual address:

		13..12 segment
		11.. 0 offset

	. And segments would be encoded as:

		00  (    0.. 4095)  code
		01  ( 4096.. 8191)  heap
		10  ( 8192..12287)
		11  (12288..16383)

	. So to translate virtual address 4200 (in heap)

		01 000001101000
		^             ^
		heap          104

		physicalAddress = 34K + 104 = 34920

	. If the base and bounds registers were arrays, the CPU would be
	  doing something like this:

		segment = ( virtualAddress & segmentMask ) >> nOffsetBits

		offset = virtualAddress & offsetMask

		if ( offset >= bounds[ segment ] ) { error }

		physicalAddress = base[ segment ] + offset


What about the stack?

	. Grows backwards

	. In addition to the base/bound pair, the CPU also needs to know
	  which direction a segment grows
	. Our updated view of what the CPU tracks is:

		segment  base  size  direction
		-------  ----  ----  ---------
		code     32K   2K    1
		heap     34K   2K    1
		stack    28K   2K    0

	. Thus CPU translates as:

		segment = ( virtualAddress & segmentMask ) >> nOffsetBits

		offset = virtualAddress & offsetMask

		if ( direction == 1 )
		{
			if ( offset >= bounds[ segment ] ) { error }

			physicalAddress = base[ segment ] + offset
		}
		else
		{
			offset = maxSegmentSize - offset

			if ( offset >= bounds[ segment ] ) { error }

			physicalAddress = base[ segment ] - offset
		}


	. For example, consider virtualAddress 15K (11110000000000):

		segment = 11 = stack

		offset = 110000000000 = 3072

		offset = maxSegmentSize - offset
		       = 4K - offset
		       = 1024

		physicalAddress = 28K - 1024 = 27648


Support for sharing

	. To save memory, sometimes it useful to share certain segments
	  between address spaces. In particular, code sharing.

	. This can be acheived with protection bits (read, write, execute):

		segment  base  size  direction  protection
		-------  ----  ----  ---------  ----------
		code     32K   2K    1          R . E
		heap     34K   2K    1          R W .
		stack    28K   2K    0          R W .

	. By setting a code segment to read-only, the same code can be
	  shared across multiple processes without worry


Fine-grained vs coarse-grained segmentation

	. Our three segments is coarse
	. Some early machines supported thousands, with the thinking
	  that the OS could better use memory based on which are in use

	. Supporting many segments requires further CPU support, in the
	  form of a segment table stored in memory


OS Perspective

	~~~ first ... ~~~

	. What should the OS do on a context switch?
	. Segment registers must be saved and restored


	~~~ second ... ~~~

	. Managing free space in physical memory

	. When allocating space for the various processes' segments,
	  memory can quickly become full of little holes of free
	  space (fragmented), making it difficult to allocate new
	  segments, or grow existing ones

	. One solution is to stop whatever process is running, and
	  rearrange the exisiting segments to defragment the free space.
	. However this is expensive

	. A simpler solution is to use a free-list management algorithm
	  that tries to keep large extents of memory available
	. For example:
		. best-fit  - closest in size to desired
		. worst-fit - ?
		. first-fit
		. buddy algorithm - ?

	. The existence of many algorithms indicates that there is no
	  best way to solbe the problem
	. The only real solution is to avoid the problem altogether
	  by never allocating memory in variable-sized chunks


Summary

	. Pros
		. better support for sparse address spaces
		. minimal translation overhead

	. Cons
		. fragmented memory due to allocaation of variable-sized segments


---------------------------------------------------------------------------------------------
17) Free-space Management
---------------------------------------------------------------------------------------------

Intro

	. Small detour to look at free-space management (be it malloc
	  library managing heap, or OS managing address space of processes)

	. Free-space management can be easy when managing a space that
	  is divided into *fixed-sized* units
	. In this case, you just keep a list of these units. When a client
	  requests one of them, return the first available entry

	. It becomes difficult when the units are variable-sized
	. Requests may fail even though there is enough memory, because
	  the free sections are fragmented into little pieces, instead
	  of being contiguous


Assumptions

	. We assume a basic interface is provided:

		. void *malloc ( size_t size )
			. 'size' is number of bytes requested
			. returns a void pointer to a region of that 'size' (or greater)

		. void free ( void *ptr )
			. takes a pointer and frees the corresponding chunk
			. Since the user does not specify the chunk's size, the
			  library must be able to figure out how big a chunk is
			  when given just a pointer to it

	. The space the library manages is historically known as the "heap"

	. A "free list" is used to manage free space in the heap
	. It contains references to all of the free chunks in the heap
	. The "free list" can be any kind of data structure (not just a list)


	. We focus on external fragmentation for simplicity
	. Allocators can also have problems with internal fragmentation
	. When an allocator hands out chunks of memory bigger than
	  requested, any unasked for (and thus unused) space in a chunk
	  is considered internal fragmentation


	. We assume that once memory is handed out to a client, it
	  cannot be relocated to another location in memory
	. If a program calls malloc, it essentially "owns" the memory
	  region until it returns it with a call to free
	. Thus we assume "compaction" of free space by the library
	  is not possible...


	. We assume that the allocator manages a contiguous region

	. In some cases, an allocator could ask for that region to grow;
	  For example a user-level library could call a system call like
	  sbrk when it runs out of space
	. For simplicity, we assume the region is a fixed size


Low-level mechanisms

	. A free list contains a set of elements that describe the
	  free space still remaining in the heap.
	. Consider the following heap:

		 0.. 9 free
		10..19 used
		20..29 free

	. The free list for this heap would have two elements in it.
	. One entry to describe the first free chunk, and another the second

		head -> address :  0 -> address : 20 -> NULL
		        length  : 10    length  : 10

	. Given our assumptions, a request for anything greater than
	  10 bytes will fail, as there is no contiguous chunk available
	  to meet the request


	~~~ Splitting ~~~

	. What happens when we request for something smaller than 10?

	. Assume we have a request for just a single byte of memory
	. In this case, the allocator will perform "splitting"
	. It will find a free chunk that can satisfy the request and
	  split it into two
	. The first chunk will be returned to the caller, the second
	  will remain on the list
	. Thus in our example above, if the allocator decided to use
	  the second free chunk to meet the request, malloc would
	  return 20 (the address of the allocated region) and the
	  free list would now look like:

		head -> address :  0 -> address : 21 -> NULL
		        length  : 10    length  :  9


	~~~ Coalescing ~~~

	. What happens when an application calls free(10), thus
	  returning space in the middle of our example heap?

	. If we simply add this free space back into our list,
	  we would end up with:

		head -> address :  0 -> address : 10 -> address : 21 -> NULL
		        length  : 10    length  : 10    length  :  9

	. Note while a contigous block of 20 bytes is free, it is
	  seemingly divided into two chunks of 10 bytes
	. Thus if a user requests 20 bytes, the request will fail

	. To avoid this, the allocator needs to coalesce free space
	  when a chunk of memory is freed
	. When returning a free chunk, it should look at the adjacent
	  chunks to see if they are free.
	. If they are free, the allocator should merge them into one
	  single chunk

	. With coalescing, our list would look like:

		head -> address :  0 -> address : 21 -> NULL
		        length  : 20    length  :  9


Tracking the size of allocated regions

	. Most allocators store some information in a header block
	. The header block is usually kept just before the handed-out
	  memory chunk

	. For example, a call to malloc( 20 )

		---------  <- headerPointer
		header
		---------  <- pointer
		bytes
		allocated
		(20)
		---------

	. The header minimally contains the size of the allocated region
	. It may also contain:
		. additional pointers to speed up deallocation ?
		. a magic number to provide additional integrity checking ?
		. etc...

	. Let's assume a simple header with two fields:

		typedef struct __header_t {

			int size;
			int magic;

		} header_t;


		---------------  <- headerPointer
		size  : 20
		magic : 1234567
		---------------  <- pointer


	. When free is called, the library would use pointer arithmetic
	  to figure out where the header begins

		void free ( void *ptr )
		{
			header_t *headerPointer = ptr - sizeof( header_t );

			...
		}

	. With the headerPointer, the library can easily check if
	  the magic number matches an expected value:

		assert( headerPointer->magic == 1234567 )

	. And calculate the total size of the newly freed region
	  (the size of the header plus the space allocated to the user):

		bytesFreed = sizeof( header_t ) + headerPointer->size


	. When a user requests N bytes of memory, the library does not
	  search for a free chunk of size N
	. It looks instead for size N plus size of header


Embedding a free list

	. How do we build our free list in the space itself?

	. Assume we have a 4096 byte chunk of memory to manage

	. We first initialize our free list
	. It should have one entry, of size 4K (minus the header)
	. We assume that the heap built with some free space acquired
	  via a call to the system call 'mmap'

		typedef struct __node_t {

			int              size;
			struct __node_t *next;

		} node_t;


		// Initialize 'head' as pointer to a free space

		node_t *head = mmap(

			NULL,
			4096,
			PROT_READ | PROT_WRITE,
			MAP_ANON | MAP_PRIVATE,
			-1, 0
		);

		head->size = 4096 - sizeof( node_t );
		head->next = NULL;


	. After running this code, the list has a single free chunk
	  of size 4088

		  >  ---------------  <- head; heap base
		||   size : 4088
		||   next : 0 (NULL)
		||   ---------------  <- free space
		||   free (4088)
		  >  ---------------


	. Now imagine a call to malloc( 100 )

		typedef struct __header_t {

			int size;
			int magic;

		} header_t;


		  >  ---------------  <- heap base
		 |   size  : 100
		 |   magic : 0xBEEF
		 |   ---------------  <- returned pointer
		 |   allocated (100)
		  >  ---------------  <- head
		||   size : 3980
		||   next : 0 (NULL)
		||   ---------------
		||   free (3980)
		  >  ---------------


	. To service the request, the library first finds a chunk
	  large enough to accomodate the request
	. It then spits the chunk into two:
		. one big enough to service the request plus a header
		. one as the remaining free chunk

	. Fast forward a few more allocations:

		  >  ---------------  <- heap base (16384)
		 |   size  : 100
		 |   magic : 0xBEEF
		 |   ---------------
		 |   allocated (100)
		  >  ---------------  <- (16492)
		 |   size : 100
		 |   magic : 0xBEEF
		 |   ---------------
		 |   allocated (100)
		  >  ---------------  <- (16600)
		 |   size : 100
		 |   magic : 0xBEEF
		 |   ---------------
		 |   allocated (100)
		  >  ---------------  <- head (16708)
		||   size : 3764
		||   next : 0 (NULL)
		||   ---------------
		||   free (3764)
		  >  ---------------


	. Imagine a program calls free( 16500 )
	. The library figures out the size of the free region and
	  adds it back to the free list
	. Assuming we insert at the head of the free list, the heap
	  now looks like:

		  >  ---------------  <- heap base (16384)
		 |   size  : 100
		 |   magic : 0xBEEF
		 |   ---------------
		 |   allocated (100)
		  >  ---------------  <- head (16492)
		||   size : 100
		||   next : 16708
		||   ---------------
		||   free (100)
		  >  ---------------  <- (16600)
		 |   size : 100
		 |   magic : 0xBEEF
		 |   ---------------
		 |   allocated (100)
		  >  ---------------  <- (16708)
		||   size : 3764
		||   next : 0 (NULL)
		||   ---------------
		||   free (3764)
		  >  ---------------


	. Now assume the last two in-use chunks are freed
	. Without coalescing, the heap might look like:

		  >  ---------------  <- heap base (16384)
		||   size  : 100
		||   next  : 16492
		||   ---------------
		||   free (100)
		  >  ---------------  <- (16492)
		||   size : 100
		||   next : 16708
		||   ---------------
		||   free (100)
		  >  ---------------  <- head (16600)
		||   size : 100
		||   next : 16384
		||   ---------------
		||   free (100)
		  >  ---------------  <- (16708)
		||   size : 3764
		||   next : 0 (NULL)
		||   ---------------
		||   free (3764)
		  >  ---------------


	. Instead of:

		  >  ---------------  <- head; heap base (16384)
		||   size  : 4088
		||   next  : 0 (NULL)
		||   ---------------
		||   free (4088)
		  >  ---------------


Growing the heap

	. What to do when heap runs out of space?

	. Simplest approach is to fail, and return NULL

	. Another approach is to request more memory from the OS using
	  system calls like 'sbrk', and use it to grow the heap

	. To service sbrk, the OS:
		. finds free physical pages,
		. maps them into the address space of the requesting process,
		. and then returns the value of the new program end
		  (aka end of the process's data segment, aka heapEnd)
	. At that point, a larger heap is available and can be used to
	  service the malloc request


	. JK
		. This

			  >  ---------------  <- heap base (16384)
			 |
			 |   ...
			 |
			  >  ---------------  <- 20462
			||   size : 10
			||   next : 0 (NULL)
			||   ---------------
			||   free (10)
			  >  ---------------


		. Might become (after 4K allocated by sbrk)

			  >  ---------------  <- heap base (16384)
			 |
			 |   ...
			 |
			  >  ---------------  <- tail (20462)
			||   size : 4106
			||   next : 0 (NULL)
			||   ---------------
			||   free (4106)       // 10 + 4096
			  >  ---------------


		. This assumes sbrk basically does:

			heapEnd = heapEnd + 4096


		. As such it seems if want the library to be able to grow
		  the heap:

			. (1) Chunks returned by 'free' must be added to the head
			  of the list in order to preserve the tail's location
			  at the end of the heap segment

			. (2) If the tail is the only chunk big enough to fulfill
			  a malloc request, it can only service the request if
			  it is less than or equal to:

				tail->size - ( sizeof( node_t ) + 1 byte )

			. I.e. after the allocation, a tail will still exist...
			. Otherwise, a call to sbrk to grow the heap must first be
			  made before servicing the malloc


Basic strategies

	. Ideal allocator is fast and minimizes fragmentation

	. However, there is no "best" approach


	~~~ Best fit ~~~

	. Search through list and find chunks that are big or bigger than
	  requested
	. Return the one that is smallest among the group of candidates

	. Tries to reduce wasted space
	. Naive implementations pay a heavy perfrormance penalty when
	  performing an exhaustive search for a block


	~~~ Worst fit ~~~

	. Find the largest free chunk available and return the requested
	  amount from it

	. Tries to leave big chunks free


	~~~ First fit ~~~

	. Simply finds first block that is big enough

	. Has the advantage of speed
	. Sometimes pollutes the beginning of the free list with small objects

	. address-based ordering ??


	~~~ Next fit ~~~

	. Instead of always starting a first-fit search at the beginning
	  of the list, this approach keeps an extra pointer to the
	  location in the list where one was looking last
	. The idea is to spread searches for free space throught the
	  list more uniformly

	. Performance is quite similar to first-fit


Other approaches

	. revisit


---------------------------------------------------------------------------------------------
18) Paging: Introduction
---------------------------------------------------------------------------------------------

Overview

	. A process's address space is divided into fixed-sized units
	  referred to as "pages"

		address space

			-------
			page 0
			-------
			page 1
			-------
			page 2
			-------
			page 3
			-------

	. Correspndingly, physical memory is divided into fixed-sized
	  units referred to as "page frames"

		physical memory

			--------------------
			OS                     page 0
			--------------------
			free                   page 1
			--------------------
			page 3 of process_1    page 2
			--------------------
			page 0 of process_1    page 3
			--------------------
			free                   page 4
			--------------------
			page 2 of process_1    page 5
			--------------------
			free                   page 6
			--------------------
			page 1 of process_1    page 7
			--------------------

	. Paging is more flexible
		. No assumptions are made about how a process will use
		  the address space (for example about existence of, and
		  direction the heap and stack grow)
	. Paging is simpler with regards to free-space management
		. Fixed-sized slots

	. In our example, to allocate space for a process's address
	  space, the OS just needs to find four free physical pages


	~~~ page table ~~~

	. To record where each virtual page of an address space is
	  placed in physical memory, the OS usually keeps a *per-process*
	  data structure known as a "page table"
	. For our example, process_1's page table would look like:

		virtual page 0 : physical page 3
		virtual page 1 : physical page 7
		virtual page 2 : physical page 5
		virtual page 3 : physical page 2


	~~~ translation ~~~

	. To translate a virtual address, we first split it into
	  two components: virtual page number and offset

	. For our example, we assume each page is 16 bytes, and
	  each address space 64 bytes (16x4)
	. As such, we need 6 bits in our virtual address (2^6 = 64)
	. Since each address space has 4 pages, we use the two
	  upper bits of the address to indicate the virtual page number

		5..4 - virtual page number
		3..0 - offset

	. For example, virtual address 21 (01_0101) becomes:

		virtual page number = 01 = 1

		offset = 0101 = 5

		physical page number = page_table[ virtual page number ]
		                     = page_table[ 1 ]
		                     = 7

		physical address = physical page number * page size + offset
		                 = 7 * 16 + 5
		                 = 117


Where are page tables stored?

	. Consider a 32-bit address space with 4K pages.
	. The virtual address splits into a 12-bit offset (4K) and
	  20-bit virtual page number (VPN)
	. A 20-bit VPN implies a page table with 1M entries per process
	. Assuming we use 4 bytes (why?) per page table entry, we would
	  need 4M of memory per page table!
	. Too big!

	. The page table of the currently running process is stored
	  somewhere in memory that the OS manages


What's actually in the page table?

	. The simplest form possible is a linear page table, aka an array.
	. The OS would index the array by the VPN to get the PPN
	. For now, we assume this linear structure

	. In a page table entry, typically have the following bits:

		. valid -  whether the translation is valid.
		           For example, when a program starts running, it will
		           have chunks of its address space that have yet to be
		           allocated physical memory (ex. between stackEnd and
		           heapEnd)

		. protection - whether page can be read, write, execute

		. present - whether the page is on memory or disk
		            This comes into play when we have to swap parts of
		            the address space to disk to support those that are
		            larger than physical memory

		. dirty - whether the page has been modified since it was
		          brought into memory

		. accessed - (aka referenced) whether a page has been accessed.
		             Useful for determining which pages are popular and
		             should thus be kept in memory

	. x86 page table entry:

		31..12 physical page number
		11.. 9 unused
		     8 .
		     7 .
		     6 dirty
		     5 accessed
		     4 .
		     3 .
		     2 user/supervisor - can user-mode processes access the page
		     1 read/write
		     0 present


Too slow

	. Consider the case of one instruction:

		movl 21, %eax

	. To fetch the desired data, the CPU must first translate the
	  virtual address (21) into the correct physical address
	. To do the translation, the CPU must first fetch the proper
	  page table entry
	. To do so, the CPU must know where the page table is for the
	  currently running process. For now, we assume that a
	  "page table base" register holds the physical address.
	. Thus:

		// Fetch page table entry

		virtualPageNumber = ( virtualAddress & vpnMask ) >> nOffsetBits

		pageTableEntryAddress = pageTableBaseRegister + ( virtualPageNumber * sizeof( pageTableEntry ) )

		pageTableEntry = memory[ pageTableEntryAddress ]


		if ( pageTableEntry.valid == false ) { error }

		if ( canAccess( pageTableEntry.protection ) == false ) { error }


		// Get physical page number

		physicalPageNumber = ( pageTableEntry & ppnMask ) >> nOffsetBits


		// Translate the address

		offset = virtualAddress & offsetMask

		physicalAddress = ( physicalPageNumber << nOffsetBits ) | offset


	. For example:

		virtualPageNumber = ( 21 & 11_0000 ) >> 4
		                  = 1

	. Paging requires us to perform an extra memory reference in
	  order to get the relevant page table entry
	. This is slow!


Memory Trace

	. Consider program:

		void main ( ... )
		{
			int i;
			int array [ 1000 ];

			for ( i = 0; i < 1000; i += 1 )
			{
				array[ i ] = 0;
			}
		}

	. Which becomes assembly instructions:

		1024: movl $0x0, ( %edi, %eax, 4 )  // memory[ edi + eax * 4 ] = 0
		1028: incl %eax                     // eax += 1
		1032: cmpl $0x03e8, %eax            // does eax == 1000
		1036: jne  0x1024                   // if no, loop

	. Assume:

		. virtual address space size of 64K
		. page size of 1K
		. linear (array-based) page table located at physical address 1K
		. code lives in virtual address 1024 (VPN 1)
		. array lives in virtual addresses 40000..44000 (VPN 39..42)
		. VPN  1 matches to PPN 4
		. VPN 39 matches to PPN 7
		. VPN 40 matches to PPN 8
		. VPN 41 matches to PPN 9
		. VPN 42 matches to PPN 10

	. When the program runs, each instruction will generate two
	  memory references:
		. one to find the PTE
		. one to fetch the instruction
	. There is also one explicit memory reference with the 'mov'.
	  It adds another two memory references:
	  	. one to find the PTE of target
	  	. one to do the 'mov'
	. Thus there are a total of 10 accesses per iteration:
		. four instruction fetches
		. one explicit update of memory
		. five page table accesses to translate

	. Memory access for the first three iterations looks like:

		instruction   virtual   physical
		              address   address
		-----------   -------   --------

		mov
		                        1028  // page table access
		                  1024  4096  // instruction memory access
		                        1180  // page table access
		                 40000  7232  // heap access

		inc
		                        1028  // page table access
		                  1028  4100  // instruction memory access
		cmp
		                        1028  // page table access
		                  1032  4104  // instruction memory access
		jne
		                        1028  // page table access
		                  1036  4108  // instruction memory access
		--

		mov
		                        1028  // page table access
		                  1024  4096  // instruction memory access
		                        1180  // page table access
		                 40004  7236  // heap access

		inc
		                        1028  // page table access
		                  1028  4100  // instruction memory access
		cmp
		                        1028  // page table access
		                  1032  4104  // instruction memory access
		jne
		                        1028  // page table access
		                  1036  4108  // instruction memory access
		--

		mov
		                        1028  // page table access
		                  1024  4096  // instruction memory access
		                        1180  // page table access
		                 40008  7240  // heap access

		inc
		                        1028  // page table access
		                  1028  4100  // instruction memory access
		cmp
		                        1028  // page table access
		                  1032  4104  // instruction memory access
		jne
		                        1028  // page table access
		                  1036  4108  // instruction memory access


---------------------------------------------------------------------------------------------
19) Paging: Translation Lookaside Buffers
---------------------------------------------------------------------------------------------

Intro

	. Going to memory for translation information before every
	  instruction fetch is prohibitively slow
	. To speed up, we use a "translation lookaside buffer"
	. A better name would be "address translation cache"

	. The TLB is part of the CPU's memory management unit
	. It is a hardware cache of popular virtual-to-physical translations

	. On each virtual memory reference, the CPU first checks the
	  TLB to see if it contains the desired translation
	. If so, the translation is performed quickly without having to
	  consult the page table


Basic Algorithm

	. Assuming a linear page table:

		// Get virtual page number

		virtualPageNumber = ( virtualAddress & vpnMask ) >> nOffsetBits


		// Check if TLB holds the corresponding PPN

		( success, tlbEntry ) = tlbLookup( virtualPageNumber )


		// TLB hit

		if ( success )
		{
			if ( canAccess( tlbEntry.protection ) == false ) { raiseException( protection ) }

			// Translate the address

			offset = virtualAddress & offsetMask

			physicalAddress = ( tlbEntry.physicalPageNumber << nOffsetBits ) | offset
		}

		// TLB miss

		else
		{
			// Fetch page table entry

			pageTableEntryAddress = pageTableBaseRegister + ( virtualPageNumber * sizeof( pageTableEntry ) )

			pageTableEntry = memory[ pageTableEntryAddress ]


			if ( pageTableEntry.valid == false ) { raiseException( invalidPage ) }

			if ( canAccess( pageTableEntry.protection ) == false ) { raiseException( protection ) }


			// Add the entry to the TLB

			tlbInsert( virtualPageNumber, pageTableEntry.physicalPageNumber, pageTableEntry.protection )

			retryInstruction()
		}

	. We check if the TLB holds the translation for the virtual page number
	. If it does (TLB hit), we use it to gate the physical address
	. If it doesn't (TLB miss), we:
		. retrieve the page table entry and add it to the TLB
		. Then try the instruction again. This time it will yield a TLB hit


	. The TLB solution works on the premise that the most common
	  translations will be found in the TLB (hits)
	. A miss incurrs the high cost of paging


Example: Accessing an array

	. Assume:

		. 16-byte pages
		. 8-bit virtual address space
			. thus 4-bit VPN, 4-bit offset
		. array starts at virtual address 100 (VPN 6, offset 4)

	. Array looks like this in memory:

		-----  VPN 0
		...
		-----  VPN 6
		.
		a[0]
		a[1]
		a[2]
		-----  VPN 7
		a[3]
		a[4]
		a[5]
		a[6]
		-----  VPN 8
		a[7]
		a[8]
		a[9]
		.
		-----  VPN 9
		...
		-----

	. Consider program:

		void main ( ... )
		{
			int i, sum;
			int array [ 10 ];

			sum = 0;

			for ( i = 0; i < 10; i += 1 )
			{
				sum += array[ i ];
			}
		}

	. We pretend that the only memory accesses that the loop generates
	  are to the array

	. When the first array element is accessed a[0], we get a TLB miss.
	. VPN 6 is thus added to the TLB
	. When the second element is accessed a[1], we get a hit.
	. Likewise a hit for a[2]
	. This is because they both live on the same page as a[0]

	. When a[3] is accessed, we get a TLB miss.
	. However, the subsequent access (a[4], a[5], a[6]) are hits.

	. When a[7] is accessed, we get a TLB miss
	. etc.

	. The hit rate is 70% ( 7 hits / 10 accesses )
	. The TLB has improved performance due to spatial locality.
	. The elements of the array are packed tightly into pages, and
	  thus accessing only the first in a page generates a miss

	. If the program runs again soon after, we would see an
	  even higher hit rate (100%)
	. The TLB performance would improve due to temporal locality
	  i.e. the relevant translations are still in the TLB


Who handles TLB misses?

	~~~ Old ~~~

	. In early architectures, the CPU did
	. The CPU knew where the page table was in memory (page table register)
	  as well as its exact format.
	. On a miss:
		. the CPU would walk the page table,
		. find the desired pte,
		. update the TLB with the translation,
		. and retry the instruction

	. An example of this "hardware-managed TLB" is the x86 architecture
		. it uses a fixed multi-level page table (explained in
		  next chapter)
		. CR3 register holds base address of current page table


	~~~ New ~~~

	. Modern architectures (ex MIPS, SPARC) use a "software-managed TLB"

	. On a TLB miss:
		. the CPU simply raises an exception,
		. which pauses the current instruction stream,
		. raises the privilege level to kernel mode,
		. and jumps to a trap handler
	. The trap handler:
		. looks up the translation from the page table
		. uses "privileged" instructions to update the TBL
		. and returns from the trap.
	. At this point, the hardware retries the instruction


	. The return-from-trap for a TLB miss is slightly different from
	  that for servicing a system call
	. For a system call, execution should resume at the instruction
	  immediately following the call
	. For a TLB miss, execution should resume at the instruction
	  that caused the trap
	. Thus depending on how the trap was caused, a different PC must
	  be saved


	. When running the TLB miss trap handler code, the OS needs
	  to be extra careful not to cause an infinite chain of misses
	  to occur
	. Possible solutions:
		. Keep the TLB miss handler code in physical memory where
		  it is unmapped and not subject to address translation
		. Reserve some entries in the TLB for permanently-valid
		  translations of the handler code (i.e. always a hit)


	. The hardware passes off handling a miss to the OS by
	  raising an exception:

		// Get virtual page number

		virtualPageNumber = ( virtualAddress & vpnMask ) >> nOffsetBits


		// Check if TLB holds the corresponding PPN

		( success, tlbEntry ) = tlbLookup( virtualPageNumber )


		// TLB hit

		if ( success )
		{
			if ( canAccess( tlbEntry.protection ) == false ) { raiseException( protection ) }

			// Translate the address

			offset = virtualAddress & offsetMask

			physicalAddress = ( tlbEntry.physicalPageNumber << nOffsetBits ) | offset
		}

		// TLB miss

		else
		{
			raiseException( TLBMiss )
		}


TLB Contents

	. A typical TLB might have 32, 64, or 128 entries and be fully associative

	. Fully associative means that a translation can be anywhere in
	  the TLB and that the hardware? will search the entire TLB
	  in parallel to find the desired translation

	. A TLB entry might look like:

		VPN | PPN | other bits

	. The VPN is present because in a fully associative TLB, the
	  translation could end up in any location

	. Other bits can include:
		. valid      - whether the entry holds a valid translation
		. protection - how a page can be accessed (inherited from
		               pte protection bits)
		. dirty - ?
		. address-space identifier - ?


	~~~ TLB valid bit ~~~

	. A pte marked invalid means that the page has not been allocated
	  to a process
	. The typical response when an invalid page is accessed is to
	  trap to the OS which responds by killing the process

	. A TLB entry marked invalid means that the entry does not have
	  a valid translation
	. For example, when a system boots, it is common to set all TLB
	  entries as invalid since no translations have yet to be cached
	. Once programs start running and accessing their virtual
	  address spaces, the TLB is slowly populated with valid entries

	. When performing a context switch, all TLB entries can be
	  marked as invalid, to ensure that the about-to-be-run process
	  does not accidentally use a virtual-to-physical translation
	  from a previous process


Context switches

	. The TLB contains V2P translations that are only valid for 
	  the currently running process
	. When switching from one process to another, the CPU or OS or
	  both must ensure that the about-to-run process does not use
	  the v2P translations of the previous

	. Consider process_1 with VPN 10 mapped to PPN 100
	. Consider process_2 with VPN 10 mapped to PPN 250
	. If entries for both processes are present in the TLB, the
	  hardware can't distinguish which entry is meant for which
	  process:

		VPN  PPN  valid  protection
		---  ---  -----  ----------
		10   100  1      rwx
		10   250  1      rwx


	~~~ Flush ~~~

	. One possible solution to this problem, is to simply *flush* the
	  TLB on context switches, emptying it before running the next process

	. A flush simply sets all valid bits to 0

	. On a software-based system, this can be accomplished with
	  an explicit (and privileged) CPU instruction
	. For example ??

	. On a hardware-based system, the flush could be enacted when
	  the page table base register is changed

	. There is a cost however.
	. Each time a process runs, it must incur TLB misses until it
	  builds up the TLB entries again
	. If the OS switches between processes frequently, the speed
	  benefits realized by using the TLB diminish


	~~~ Address space identifier ~~~

	. Some systems add hardware support to enable sharing of the
	  TLB across context switches
	. Specifically, they provide an address space identifier (ASID) bit

	. You can think of the ASID as a process identifier (PID), but
	  usually it has fewer bits (8 vs 32 for a PID) ??

	. The TLB would look like:

		VPN  PPN  valid  protection  ASID
		---  ---  -----  ----------  ----
		10   100  1      rwx         1
		10   250  1      rwx         2

	. The CPU now needs to know which process is currently running
	  in order to perform the appropriate translation
	. Thus the OS must, on a context switch, set a privileged
	  register to the ASID of the current process


Replacement policy

	. When placing an entry into the TLB, we have to replace an old one

	. One common approach is to evict the least-recently-used (LRU)

	. Another approach is to evict randomly.

	. We will look at policies in more detail later when tackling
	  swapping pages to disk


A real TLB entry

	. Lets look at MIPS R4000

	. Simplified TLB entry

		31..24  ASID
		23..20  .
		    19  global
		18.. 0  VPN

		    31  .
		    30  valid
		    29  dirty
		28..26  coherence
		25.. 2  PPN
		 1.. 0  .

	. The R4000 supports a 32-bit address space with 4K pages
	. As such, would expect 12-bit offset and 20-bit VPN
	. However, the TLB entry only has a 19-bit VPN
	. This is because in the R4000, user addresses will only come
	  from half the address space. The other half is reserved for
	  the kernel

	. The VPN translates to a 24-bit PPN ??
	. It can thus support systems with up to 64G (2^24 x 4K) of
	  physical memory

	. The ASID is used to distinguish between address spaces
	. What should the OS do when there are more than 256 (2^8)
	  processes running at a time ??

	. The global bit is used for pages that are globally shared
	  among processes
	. When the global bit is set, the ASID is ignored

	. The coherence bit is used to determine how a page is
	  cached by the hardware ??

	. The diry bit is marked when the page has been written to...

	. There is a page mask field (not shown) which supports
	  multiple page sizes


	. Most of the TLB entries are used by the currently running
	  user process. However, a few are reserved for the OS.
	. A "wired" register can be set by the OS, to tell the CPU
	  how many slots of the TLB to reserve for the OS
	. The OS uses these reserved mappings for code and data that
	  it wants to access during critical times whewre a TLB miss
	  would be problematic (ex. TLB miss handler code)

	. Because the R4000 TLB is software managed, there need to be
	  instructions to update the TLB:

		. TLBP  - probes the TLB to see if a particular translation
		          is present
		. TLBR  - reads the contents of a TLB entry into registers
		. TLBWI - replaces a specific TLB entry
		. TLBWR - replaces a random TLB entry

	. The OS uses these instructions to manage the TLB's contents


Summary

	~~~ TLB coverage problem ~~~

	. When the number of pages a program accesses in a short
	  period of time exceeds the number of TLB entries

	. One solution is to include support for larger pages.
	. By mapping key data structures into regions of the program's
	  address space that are mapped by larger pages...


	~~~ Physicall-indexed cache ~~~

	. ??


---------------------------------------------------------------------------------------------
20) Paging: Smaller Tables
---------------------------------------------------------------------------------------------

Simple solution: bigger pages

	. We could reduce the size of the page table by using bigger pages
	. Downside, big pages lead to waste within each page


Hybird approach: paging and segments

	. Consider the case where we have an address space in which
	  the used portions of the heap and stack are small

	. Assuming:
		. a 16K address space with 1K pages
		. a single code page (VPN 0, PPN 10)
		. a single heap page (VPN 4, PPN 23)
		. two stack pages (VPN 14, PPN 28) (VPN 15, PPN 4)

	. The page table might look like:

		VPN  PPN  valid  protection  present  dirty
		---  ---  -----  ----------  -------  -----
		0    10   1      R.X         1        0
		1    .    0      ...         .        .
		2    .    0      ...         .        .
		3    .    0      ...         .        .
		4    23   1      RW.         1        1
		5    .    0      ...         .        .  
		6    .    0      ...         .        .  
		7    .    0      ...         .        .  
		8    .    0      ...         .        .  
		9    .    0      ...         .        .  
		10   .    0      ...         .        .  
		11   .    0      ...         .        .  
		12   .    0      ...         .        .  
		13   .    0      ...         .        .  
		14   28   1      RW.         1        1
		15   4    1      RW.         1        1

	. Most of the page table is unused

	. revisit...


Multi-level page tables

	. Chop up the page table into page-sized units
	. If an entire page of a PTEs is invalid, don't allocate that
	  page of the page table...

	. To track whether a page of the page table is valid (and if
      valid, where it is in memory), a new structure is used -
      page directory
    . The page directory can be used to tell you where a page of
      the page table is, or that the entire page contains no
      valid entries


    . For example:

		Linear

			page table register : 201

			page table:

				valid  PPN
				-----  ---  <-
				1      12     |
				1      13     | PPN 201
				0      .      |
				1      100    |
				- - - - - -  <-
				0      .      |
				0      .      | PPN 202
				0      .      |
				0      .      |
				- - - - - -  <-
				0      .      |
				0      .      | PPN 203
				0      .      |
				0      .      |
				- - - - - -  <-
				0      .      |
				0      .      | PPN 204
				1      86     |
				1      15     |
				- - - - - -  <-


		Multi-level

			page directory register : 200

			page directory:

				valid  PPN
				-----  ---  <-
				1      201    |
				0      .      | PPN 200
				0      .      |
				1      204  <-


			page table:

				valid  PPN
				-----  ---  <-
				1      12     |
				1      13     | PPN 201
				0      .      |
				1      100    |
				- - - - - -  <-
				              |
				     not      | PPN 202
				  allocated   |
				              |
				- - - - - -  <-
				              |
				     not      | PPN 203
				  allocated   |
				              |
				- - - - - -  <-
				0      .      |
				0      .      | PPN 204
				1      86     |
				1      15     |
				- - - - - -  <-


	. In the multi-level, unused pages of the page table are not
	  allocated memory (the memory can be used for other things)

	. A multi-level "makes parts of a linear page table disappear"
	  and tracks which parts are or aren't allocated in the
	  page directory

	. The page directory in a simple two-level table, contains one
	  entry per page of the page table
	. A page directory entry has at minimum:
		. a valid bit
		. a PPN
	. If a PDE's valid bit is set, it means that at least one of the
	  pages in the page it points to is valid


	~~~ Advantages ~~~

	. If carefully constructed, each portion of the page table fits
	  neatly within a page, making it easier to manage memory
	. The OS can simply grab the next free page when it needs to
	  allocate or grow a page table

	. The page table does not need to reside contiguously in
	  physical memory
	. The level of indirection added with the page directory allows
	  us to place chunks wherever we want


	~~~ Disadvantages ~~~

	. On a TLB miss, two loads from memory will be required to get
	  the right translation:
		. one to fetch the relevant PDE
		. one to fetch the relevant PTE

	. More complex PTE lookup logic


Detailed example

	. Assume:
		. address space of size 16K with 64-byte pages
		. 14-bit virtual address space
			. 8 bits for VPN     (16K / 64bytes/page = 256 pages )
			. 6 bits for offset
		. code  - VPN 0, 1
		. stack - VPN 4, 5
		. heap  - VPN 254, 255

	. A linear page table would have 256 (2^8) entries, even
	  if only a small portion of the address space is in use

	. To build a two-level page table, we start with a linear
	  table and break it up into page-sized units

	. PTEs held in each page-sized unit:
		64 bytes/unit  /  4 bytes/PTE = 16 PTEs/unit
	. Number of page-sized units:
		256 PTEs  /  16 PTEs/unit = 16 units

	. Size of page table:
		256 PTEs * 4 bytes/PTE = 1024 bytes
	. Number of page-sized units:
		1024 bytes / 64 bytes = 16

	. The page directory needs one entry per page-sized unit
	  (of the page table)
	. There are 16 page sized units in our example, so 4 bits
	  of the VPN are used for the page directory index
	. The remaining bits of the VPN form the page table index

	. I.e.

		         VPN                  offset
		 _______________________ _________________
		v                       v                 v
		|13|12|11|10| 9| 8| 7| 6| 5| 4| 3| 2| 1| 0|
		^           ^           ^
		|___________|___________|
		   PDindex      PTindex


	. The PDindex can be used to find the address of the PDE
	  containing the page table unit containing the desired PTE

		pageDirectoryEntryAddress = pageDirectoryBaseRegister + ( pageDirectoryIndex * sizeof( pageDirectoryEntry ) )

		pageDirectoryEntry = memory[ pageDirectoryEntryAddress ]


	. To find the desired PTE, we index into the page table unit
	  retrieved from the PDE using the remaining bits of the VPN

		pageTableEntryAddress = ( pageDirectoryEntry.physicalPageNumber << nOffsetBits ) + ( pageTableIndex * sizeof( pageTableEntry ) )

		pageTableEntry = memory[ pageTableEntryAddress ]


	~~~ ... ~~~

	. Physical memory might look like:

		page directory:

			valid  PPN
			-----  ---
			1      100   // VPN 0..15
			0      .
			0      .
			0      .
			0      .
			0      .
			0      .
			0      .
			0      .
			0      .
			0      .
			0      .
			0      .
			0      .
			0      .
			1      101   // VPN 240..255


		page table (section at PPN 100):

			valid  protection  PPN
			-----  ----------  ---
			1      R.X         10    // code  (VPN 0)
			1      R.X         23    // code  (VPN 1)
			0      ...         .
			0      ...         .
			1      RW.         80    // heap  (VPN 4)
			1      RW.         59    // heap  (VPN 5)
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .	


		page table (section at PPN 101):

			valid  protection  PPN
			-----  ----------  ---
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			0      ...         .
			1      RW.         55    // stack  (VPN 254)
			1      RW.         45    // stack  (VPN 255)	


	. Instead of allocationg the full 16 pages of the page table,
	  we allocate only 3 pages:
		. one for the page directory
		. two for the pages with valid mappings

	. Suppose we want get the physical address of virtual address 0x3F80:

		11 1111 1000 0000

		offset = 00 0000 = 0

		vpn = 1111 1110 = 254

		pdIdx = 1111 = 15
		ptIdx = 1110 = 14

		pde.ppn = pd[ pdIdx ].ppn
		        = pd[ 15 ].ppn
		        = 101 (see diagram above)

		pte.ppn = pt_unit[ ptIdx ].ppn
		        = pt_unit_101[ 14 ].ppn
		        = 55 (see diagram above)

		physAddr = ( pte.ppn << nOffsetBits ) | offset
		         = ( 55 << 6 ) | 0
		         = 3520


Logic summary

	. Assuming a hardware-managed TLB:

		// Get virtual page number

		virtualPageNumber = ( virtualAddress & vpnMask ) >> nOffsetBits


		// Check if TLB holds the corresponding PPN

		( success, tlbEntry ) = tlbLookup( virtualPageNumber )


		// TLB hit

		if ( success )
		{
			if ( canAccess( tlbEntry.protection ) == false ) { raiseException( protection ) }

			// Translate the address

			offset = virtualAddress & offsetMask

			physicalAddress = ( tlbEntry.physicalPageNumber << nOffsetBits ) | offset
		}

		// TLB miss

		else
		{
			// Fetch page directory entry

			pageDirectoryIndex = ( virtualPageNumber & PDindexMask ) >> ( nPTindexBits + nOffsetBits )

			pageDirectoryEntryAddress = pageDirectoryBaseRegister + ( pageDirectoryIndex * sizeof( pageDirectoryEntry ) )

			pageDirectoryEntry = memory[ pageDirectoryEntryAddress ]


			if ( pageDirectoryEntry.valid == false ) { raiseException( invalidPage ) }


			// Fetch page table entry

			pageTableIndex = ( virtualPageNumber & PTindexMask ) >> nOffsetBits

			pageTableEntryAddress = ( pageDirectoryEntry.physicalPageNumber << nOffsetBits ) + ( pageTableIndex * sizeof( pageTableEntry ) )

			pageTableEntry = memory[ pageTableEntryAddress ]


			if ( pageTableEntry.valid == false ) { raiseException( invalidPage ) }

			if ( canAccess( pageTableEntry.protection ) == false ) { raiseException( protection ) }


			// Add the entry to the TLB

			tlbInsert( virtualPageNumber, pageTableEntry.physicalPageNumber, pageTableEntry.protection )

			retryInstruction()
		}


More than two levels

	. Assume:
		. a 30-bit virtual address space
		. 512 byte page size
		. virtual address splits into:
			. 21-bit VPN
			. 9-bit offset

	. PTEs per page
		512 bytes/page  /  4 bytes/PTE = 128 PTEs/page

	. Thus we need 14 bits for the PDindex
		2^21 PTEs  /  128 PTEs/page = 16384 pages  (2^14)

	. And 7 bits for the PTindex
		128 PTEs/page (2^7)

	. If our page directory has 2^14 entries, it spans 128 pages
		2^14 PDEs * 4 bytes/PDE  /  512 bytes/page = 128 pages

		                          VPN                                            offset
		 ______________________________________________________________ __________________________
		v                                                              v                          v
		|29|28|27|26|25|24|23|22|21|20|19|18|17|16|15|14|13|12|11|10| 9| 8| 7| 6| 5| 4| 3| 2| 1| 0|
		^                                         ^                    ^
		|_________________________________________|____________________|
		                  PDindex                        PTindex


	. With such a big page directory, our goal of using a
	  multi-level page table to reduce memory use is foiled

	. To remedy this, we split the page directory itself into
	  multiple pages, and add another page directory on top to
	  point to the pages of the page directory...

		                          VPN                                            offset
		 ______________________________________________________________ __________________________
		v                                                              v                          v
		|29|28|27|26|25|24|23|22|21|20|19|18|17|16|15|14|13|12|11|10| 9| 8| 7| 6| 5| 4| 3| 2| 1| 0|
		^                    ^                    ^                    ^
		|____________________|____________________|____________________|
		      PDindex 0            PDindex 1            PTindex


	. Now when indexing the upper-level page directory, we use the
	  topmost bits of the VPN (PDindex 0)
	. If the PDE is valid, the second-level page directory is consulted
	. If the second-level PDE is valid, the page table is consulted
	. If the PTE is valid, we get out physical address...


Inverted page table

	. Instead of many page tables (one per process), a single page
	  table is used that has an entry for each physical page of the
	  system
	. The PTE tells us which process is using the page, and the
	  virtual page that maps to it

	. A linear scan to find the PTE would be expensive ?? so a
	  has table is often built over the base structure to speed up
	  lookups...??

	. TODO


Swapping pages to disk

	. Even with our many tricks to reduce the size of page tables,
	  it is possible that they may be too big to fit into memory
	  all at once
	. Some systems place such page tables in "kernel virtal memory",
	  thus ?? allowing the system to swap some of these pages to disk
	. More to come...


---------------------------------------------------------------------------------------------
21) Beyond Physical Memory: Mechanisms
---------------------------------------------------------------------------------------------

Intro

	. Assume we want to support many concurrently-running large
	  address spaces

	. The OS will need somewhere to stash away portions of address
	  spaces that currently aren't in great demand
	. This place is typically the disk


Swap space

	. First thing we need to do is reserve space on the disk for
	  moving the pages back and forth - "swap space"

	. We assume that the OS can read from and write to the swap space
	  in page-sized units
	. To do so, the OS will need to remember the "disk address" of a
	  given page

	. The size of swap space ultimately determines the maximum number
	  of memory pages that can be in use by a system at a given time
	. For now we assume it is infinite size

	. Below is an example of a 4-page physical memory, and an
	  8-page swap space
	. Three processes (1,2,3) are actively sharing physical memory
	  However, each has only a portion of their valid pages in memory
	. process_4 has all of its pages swapped out to disk, and thus
	  is not running
	. One block of swap remains free

		physical memory

			----------
			process_1    // PPN 0
			VPN 0
			----------
			process_2    // PPN 1
			VPN 2
			----------
			process_2    // PPN 2
			VPN 3
			----------
			process_3    // PPN 3
			VPN 0
			----------

		swap space

			----------
			process_1    // Block 0
			VPN 1
			----------
			process_1    // Block 1
			VPN 2
			----------
			free         // Block 2

			----------
			process_2    // Block 3
			VPN 0
			----------
			process_2    // Block 4
			VPN 1
			----------
			process_4    // Block 5
			VPN 0
			----------
			process_3    // Block 6
			VPN 1
			----------
			process_4    // Block 7
			VPN 1
			----------


	~~~ ... ~~~

	. We should not that swap space is not the only on-disk location
	  for swapping traffic...
	. For example, assume you are running a program binary...
	. The code pages from this binary are initially found on disk, and
	  when the program runs, they are loaded into memory (either all
	  at once when the program starts execution, or one page at a
	  time when needed...)

	. If the system needs to make room in physical memory for other
	  needs, it can safely re-use the memory space for these code
	  pages, knowing that it can later swap them in again from the
	  on-disk binary...??


Present bit

	. If we wish to allow pages to be swapped to disk, we need to
	  add more logic to the TLB logic
	. Specifically, when the hardware looks in the PTE, it may
	  find that the page is not present in physical memory
	. This can be determined through the PTE.present bit
	. If set, the page is present in memory, if zero it somewhere
	  on disk

	. Accessing a page that is not in physical memory is commonly
	  referred to as a "page fault"...
	. Really it should be called a "page miss"
	. Upon a page fault, the OS is invoked to service it

	. Assuming a hardware-managed TLB:

		// Get virtual page number

		virtualPageNumber = ( virtualAddress & vpnMask ) >> nOffsetBits


		// Check if TLB holds the corresponding PPN

		( success, tlbEntry ) = tlbLookup( virtualPageNumber )


		// TLB hit

		if ( success )
		{
			if ( canAccess( tlbEntry.protection ) == false ) { raiseException( protection ) }

			// Translate the address

			offset = virtualAddress & offsetMask

			physicalAddress = ( tlbEntry.physicalPageNumber << nOffsetBits ) | offset
		}

		// TLB miss

		else
		{
			// Fetch page directory entry

			pageDirectoryIndex = ( virtualPageNumber & PDindexMask ) >> ( nPTindexBits + nOffsetBits )

			pageDirectoryEntryAddress = pageDirectoryBaseRegister + ( pageDirectoryIndex * sizeof( pageDirectoryEntry ) )

			pageDirectoryEntry = memory[ pageDirectoryEntryAddress ]


			if ( pageDirectoryEntry.valid == false ) { raiseException( invalidPage ) }


			// Fetch page table entry

			pageTableIndex = ( virtualPageNumber & PTindexMask ) >> nOffsetBits

			pageTableEntryAddress = ( pageDirectoryEntry.physicalPageNumber << nOffsetBits ) + ( pageTableIndex * sizeof( pageTableEntry ) )

			pageTableEntry = memory[ pageTableEntryAddress ]


			if ( pageTableEntry.valid == false ) { raiseException( invalidPage ) }

			if ( canAccess( pageTableEntry.protection ) == false ) { raiseException( protection ) }

			if ( pageTableEntry.present == false ) { raiseException( pageFault ) }  // get page from disk


			// Add the entry to the TLB

			tlbInsert( virtualPageNumber, pageTableEntry.physicalPageNumber, pageTableEntry.protection )

			retryInstruction()
		}


Page fault(miss)

	. A page fault is handled by the OS because the hardware does
	  not have the needed info - understanding of swap space, how
	  to issue IO to the disk, etc.

	. How does the OS know where to find the desired page on disk?

	. A typical place to put the disk address of a page is in
	  its PTE...
	. The OS could use the bits in the PTE normally used for the
	  PPN to hold the "disk page number"... ??

	. When the OS receives a page fault, it looks in the PTE for
	  the disk address and issues a request to disk to fetch the page
	  into memory...

	. When the disk IO completes, the OS will then:
		. update the page table to mark the page as present
		. update the PPN field of the PTE to record the new memory location
		. then retry the instruction...

	. This next attempt may generate a TLB miss, which would then be
	  serviced, and the instruction again retried
	. One could potentially update the TLB when servicing the
	  page fault to avoid this step

	. Finally a last retry would find the translation in the TLB,
	  and proceed to fetch the desired data or instruction from memory
	  at the translated physical address


	~~~ ... ~~~

	. During the disk IO, the process will be in a "blocked" state
	. Thus the OS will be free to run other ready processes while
	  the (slow) disk fetches the requested page


What if memory is full?

	. The OS might need to "page out" one or more pages to make room
	  for the new page(s)
	. The process of picking a page to kick out is know as the
	  "page replacement policy"

	. Making the wrong decisions can cause a program to run at
	  disk-like speeds instead of memory-like speeds

	. We will look at policies in the next chapter


Page fault control flow

	. To service the page fault, the OS must roughly:
		. Find the PPN for the soon to be "paged in" page to reside in
		. If no such page exists, use a page-replacement algorithm
		  to free some pages of memory
		. Issue an IO request to read in the page from swap space
		. Update the page table
		. Retry the instruction


		// Find page in memory

		PPN = FindFreePhysicalPage()

		if ( PPN == - 1 ) 
		{
			PPN = EvictPage()  // if no free page found, kick one out
		}


		// Issue request to disk. Sleep waiting for response

		DiskRead( PTE.diskAddres, PPN )


		// Update page table entry (on return from sleep)

		PTE.present = 1
		PTE.PPN     = PPN

		retryInstruction()


When replacement really occurs

	. So far we've assumed OS only kicks out pages from memory when
	  it is full, and it needs to make room for a new page
	. However there are many reasons for the OS to keep a small
	  portion of memory free:
		. such as ?

	. Most OSs have a "high watermark" and "low watermark" to help
	  decide when to start evicting pages from memory
	. When the OS notices that few than LW pages are availble, a
	  background thread responsible for freeing memory runs
	. The thread evicts pages until there is HW pages available
	. The thread then goes back to sleep

	. This background thread is also known as the "swap daemon" or
	  "page daemon"

	. By performing a number of replacements at once, new
	  performance optimizations become possible
	. For example many systems will cluster/group a number of pages
	  and write them out at once to the swap partition

	. To work with the background paging thread, the page fault
	  handler logic should be modified slightly
	. When there are no free pages available, instead of performing
	  the page replacement directly, it should ask the swap deamon to
	  do so
	. When the thread frees up some pages, it will reawaken the
	  handler thread...


---------------------------------------------------------------------------------------------
22) Beyond Physical Memory: Policies
---------------------------------------------------------------------------------------------

Intro

	. When start running low on memory, OS has to start "paging out"
	  pages to make room for actively-used pages
	. Deciding which page(s) to evict is the "replacement policy"


Cache management
	
	. Given that main memory holds only a subset of all the VM pages
	  in the system, it can be viewed as a cache for VM pages
	. Our goal is to:
		. minimize cache misses (number of times we have to
		  fetch a page from disk)
		. maximize cache hits (number of times page found in memory)


	~~~ Types of cache misses ~~~

	. compulsory (cold-start) miss
		. occurs because the cache is empty to begin with and this
		  is the first reference to the time

	. capacity miss
		. cache ran out of space and had to evict an item

	. conflict miss
		. arises in hardware because of limits on where an item
		  can be placed in a hardware cache... due to something
		  known as set-associativity ?
		. does not arise in the VM page cache because such caches
		  are always fully-associative ? (i.e. there are no
		  restrictions on where in memory each page can be placed)


Optimal replacement policy

	. Replaces the page that will be accessed furthest in the future

	. Great for benchmarking

	. However since the future is generally not known, can't be
	  practically built

	. Let's examine how optimal does
	. Assume:
		. a program accesses the following virtual pages:
			0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1

		. cache holds 3 pages

	. optimal policy results in:

		acesss  hit/miss  evict  cache    future
		------  --------  -----  -----    ------
		0       miss      .      0        . 
 		1       miss      .      1 0      . 
 		2       miss      .      2 1 0    . 
 		0       hit       .      2 1 0    . 
 		1       hit       .      2 1 0    . 
 		3       miss      2      3 1 0    3 0 3 1 ...
		0       hit       .      3 1 0    . 
 		3       hit       .      3 1 0    . 
 		1       hit       .      3 1 0    . 
 		2       miss      0      3 1 2    2 1 ...  // can evict either 3 or 0
		1       hit       .      .        . 
 

 	. hit_rate = hits / ( hits + misses )
 	           = 6 / 11
 	           = 54%

 	. if ignore compulsory misses (first miss to a given page):

 		hit_rate = 6 / 7 = 85% 


FIFO

	. Many early systems avoided complexity of trying to approach
	  optimal and employed simple policies like FIFO

	. If FIFO, pages are simply placed in a queue when they enter
	  the system
	. When a replacement occurs, the page on the tail of the queue
	  (first-in) is evicted

	. Let's examine how FIFO does:

		acesss  hit/miss  evict  cache
		------  --------  -----  -----
		0       miss      .      0
 		1       miss      .      1 0
 		2       miss      .      2 1 0
 		0       hit       .      2 1 0
 		1       hit       .      2 1 0
 		3       miss      0      3 2 1
		0       miss      1      0 3 2
 		3       hit       .      0 3 2
 		1       miss      2      1 0 3
 		2       miss      3      2 1 0
		1       hit       .      2 1 0
 

	. hit_rate = 4 / 11 = 36%
	. hit_rate (w/out compulsory) = 4 / 7 = 57%

	. Compared to optimal (54%, 85%), FIFO does pretty bad

	. The poor performce is because FIFO does not determine the
	  importance of pages
	. Even though page 0 has been accessed a number of times, it
	  is still kicked out


Random

	. Performace varies - can be as good as optimal 


LRU

	. Use history
		. frequency - if accessed many times
		. recency   - if recently accesed

	. Works on premise of locality
	. That is, programs tend to access certain code sequences
	  (ex in a loop) and data structures (ex array accessed by
	  the loop) quite frequently
	. spacial locality  - if page accesed, likely the pages around
	                      it will also be accessed
	. temporal locality - pages accessed in near past likely to be
	                      accessed again

	. LFU - replaces least frequently used
	. LRU - replaces least recently used

	. Let's examine how LRU does:

		acesss  hit/miss  evict  cache    history
		------  --------  -----  -----    -------
		0       miss      .      0        .
 		1       miss      .      1 0      0
 		2       miss      .      2 1 0    1 0
 		0       hit       .      0 2 1    2 1 | 0
 		1       hit       .      1 0 2    0 2 | 1 0
 		3       miss      2      3 1 0    1 0 | 2 1 0
		0       hit       .      0 3 1    3 1 | 0 2 1 0
 		3       hit       .      3 0 1    0 3 | 1 0 2 1 0
 		1       hit       .      1 3 0    3 0 | 3 1 0 2 1 0
 		2       miss      0      2 1 3    1 3 | 0 3 1 0 2 1 0
		1       hit       .      1 2 3    2 1 | 3 0 3 1 0 2 1 0


	. How big is history? Or are cache "entries" re-ordered on every
	  access to reflect LRU, and then bottom-most kicked out?

	. In our example, LRU has same performance as optimal


Implementing historical algorithms

	. To implement LRU perfectly, *on each* page access, we must update
	  some data struture to move the page to the front of the list
	. Whereas to implement FIFO, the FIFO list of pages is only
	  accessed when a page is "evicted" or a new one added

	. One possible solution to reduce overhead is hardware support
	. On each page access, the hardware could update a time field
	  in memory with the current time
	. The time field could be in the per-process page table, or could
	  be in a separate array (with one entry per physical page)
	  
	. When replacing a page, the OS could scan all the time fields in
	  the system to find the LRU page
	. Unfortunately as the number of pages in a system grows, the
	  time it takes to scan the array becomes expensive


Approximating LRU

	~~~ reference bit ~~~

	. Requires hardware support in the form of a "reference" bit
	. There is one reference bit per physical? page
	. The reference bit lives somewhere in memory (ex PTE, or an array)
	. Whenever a page is used (rd/wr), the reference bit is set
	. The hardware never clears the bit - that is the responsibility
	  of the OS


	~~~ clock algorithm ~~~

	. How does the OS use the bit to approximate LRU?
	. One approach is the "clock algorithm"

	. Imagine all the pages of the system are arranged in a
	  circular list
	. A clock hand points to some random page to begin with
	. When a replacement must occur, the OS checks the reference bit
	  of the currently pointed at page
	. If the reference bit is set (1), this implies the page was recently
	  used and thus not a good candidate for replacement.
	. The refence bit of the page is cleared (0), and the clock hand is
	  incremented to the next page
	. The algorithm continues until it finds a page with its reference
	  bit cleared (0)


Considering dirty pages

	. If a page has been modified, it must be written back to disk
	  to evict it, which is expensive
	. Thus some approaches prefer to evict "clean" pages

	. To support this, the hardware should include a "dirty/modified" bit
	. The bit is set any time a page is written

	. The clock algorithm could be changed to scan for:
		. pages that are both unused and clean
		. on failing to find one, pages that are unused and dirty
		. etc.


Other VM policies

	. "When" to bring a page into memory
		. demand paging - page brought into memory when accessed
		. prefetching   - OS guesses a page is about to be used, and
		                  thus brings it into memory ahead of time

	. How the OS writes pages out to disk
		. one at a time
		. collect/cluster/group a number of pending writes together,
		  and write them to disk in one write


Thrashing

	. When the memory demands of the running processes exceeds the
	  available physical memory, and the OS finds itself constantly
	  paging

	. What to do?
	. Can use sophisticated approach and pick a subset...
	. Can use draconinan approach, and kill memory-intensive processes

	. Some versions of Linux run an "out-of-memory killer" dameon
	. When memory is oversubscribed, the dameon chooses the most
	  memory intensive process and kills it


---------------------------------------------------------------------------------------------
23) VAX/VMS Virtual Memory System
---------------------------------------------------------------------------------------------

Background

	. VAX-11 minicomputer late 1970s

	. The OS was known as VAX/VMS
	. It had to run on a broad range of machines using the VAX-11
	  architecture (from low-end to high-end)


Memory management hardware

	. VAX-11 provided a 32-bit virtual address space, divided into
	  512-byte pages
	. A virtual address thus consisted of:
		. a 23-bit VPN
		. a 9-bit offset

	. The upper two bits of the VPN were used to differentiate which
	  segment the page resided within
	. Thus the systemwas a hybrid of paging and segmentation

	. The lower half of the address space was known as "process space"
	  and was unique to each process

	. In the first half of process space (known as P0), the user
	  program is found, as well as a heap which grows downwards
	. In the second half of process space (known as P1), the stack
	  is found which grows upwards

	. The upper half of the address space was knows as "system space" (S)

	. Only half of the system space is used ??
	. Protected OS code and data reside here.
	. In this way, the OS is shared across processes

	. Diagram of the address space:

		----------------  <-   0  
		page 0: invalid     |
		----------------    |
		user code           |  user (P0)
		----------------    |
		user heap           |
		  |               <-   2^30
		  v                 |
		----------------    |
		free                |
		----------------    |  user (P1)
		  ^                 |
		  |                 |
		user stack          |
		----------------  <-   2^31
		trap tables         |
		----------------    |
		kernel data         |
		----------------    |
		kernel code         | system (s)
		----------------    |
		kernel heap         |
		  |                 |
		  v                 |
		----------------    |
		free                |
		----------------  <-   2^31 + 2^30
		unused
		----------------       2^32


	. A major concern for VMS designers was the small page size
	. The small size made simple linear page tables excessively large

	. VMS reduced the amount of memory used by page tables in two ways
	. First, they segmented the user address space into two
		. The VAX-11 hardware provides a page table for each of
		  these regions (P0, P1) per process
		. This way, no page table space?? was needed for the unused
		  portions of the address space between the stack and heap
		. The base register holds the address of the page table for
		  the segment; the bounds register holds its size (nPTEs)
	. Second, placed user page tables (P0, P1) in kernel virtual memory
		. When allocating or growing a page table, the kernel
		  allocates space out of its own virtual memory,
		  in segment S ??

	. Putting page tables in kernel virtual memory further complicates
	  address translations
	. fksl


An example of a real address space

	~~~ Null pointer ~~~

	. Page 0 is marked as inaccessible in order to provide some
	  support for detecting null-pointer accesses...
	. For example:

		int *ptr = NULL;  // 0

		*ptr = 10;  // try to store value at virtual address 0

	. The hardware will try to translate virtual address 0
	. Will incurr a TLB miss
	. When page table consulted, will find that the page is marked invalid
	. The invalid access causes the hardware to transfer control to the OS
	. The OS will likely terminate the process
	. On Unix, processes are sent a signal which allows them to react
	  to the invalid access. If uncaught however, the process is killed


	~~~ Shared kernel... ~~~

	. Kernel virtual address space (data and code) is a part of each
	  user's address space

	. On a context switch, the OS changes the P0 and P1 base/bound
	  registers to point to the appropriate page tablesof the
	  soon-to-run process
	. However it does not change the base/bound registers of S, and
	  as a result, the "same" kernel structures are mapped into each
	  user address space

	. The kernel is mapped into each address space for a number of reasons:

		. If the kernel were given its own address space, moving data
		  between user applications and the kernel would be
		  complicated and painful

		. For example when the OS is handed a pointer from a user
		  program (ex. on a 'write' system call), it is easy to copy
		  data from that pointer to its own structures...

		. With this construction:
			. The kernel appears almost as a library to applications,
			  albeit a protected one...
			. Its code can be written without worry of where the
			  data it is accessing is coming from

		. If the kernel were located entirely in physical
		  memory, it would be quite hard to do things like swap pages
		  of the page table to disk...??


	~~~ protection ~~~

	. The OS does not want user applications reading or writing OS
	  data or code
	. Thus the hardware must support different protection levels
	  for pages

	. VAX does this with protection bits in a PTE, that specify what
	  privilege level the CPU must be at in order to access the page


Page replacement

	. A PTE in the VAX contains the following bits:
		. valid
		. protection (4)
		. dirty
		. reserved for OS (5)
		. PPN

	. Since there is no reference bit, the replacement algorithm
	  used by VMS must make do without hardware support for
	  determining which pages are active...


	~~~ Emulating reference bits ~~~

	. Reference bits can be used to emulate reference bits
	. Mark all pages in page table as inaccessible, but keep around
	  information on which pages are actually accessible by the
	  process (perharps in the reserved for OS bits)
	. When a process accesses a page, it will generate a trap
	  into the OS
	. The OS will then check if the page should really be accessible,
	  and if so, revert the page to its normal protections
	. At the time of a replacement, the OS can check which pages
	  remain marked inaccessible, thus getting an idea of which were
	  recently used


	~~~ Segmented FIFO ~~~

	. Each process has a maximum number of pages it can keep in memory,
	  known as its "resident set size"
	. Each of these pages is kept on a FIFO list
	. When a program exceeds its RSS, the "first-in" page is evicted

	. Pure FIFO does not perform well...
	. To improve it performance, VMS introduced two "second-chance lists"
	  where pages are placed before getting evicted from memory
		. "clean-page list"
		. "dirty-page list"

	. When a process exceeds its RSS, a page is removed from its
	  per-process FIFO
	. If the page is clean (not modified), it is placed on the end
	  of the clean-page list
	. If dirty (modified), it is placed on the end of the
	  dirty-page list

	. If another process needs a free page, it takes the first free
	  page off of the global clean-page list
	. However, if the original process faults on that page *before*
	  it is reclaimed, the process reclaims it from the clean (or dirty)
	  list, thus avoiding a costly disk access

	. The bigger these second-chance lists are, the closer the
	  segmented FIFO algorithm performs to LRU


	~~~ Page clustering ~~~

	. Disk do better with large transfers (few big writes)...
	. VMS groups large batches of pages together from the global
	  dirty-page list, and writes them to disk all at once


Copy-on-write

	. When the OS needs to copy one page from one address space
	  to another, instead of copying it, it can map it into the
	  target address space and mark it as read-only in both
	  address spaces

	. If both address spaces only read the page, no further action
	  is taken, and thus the OS has realized a fast copy without
	  actually moving any data

	. If however one of the address spaces does indeed try to
	  write to the page, it will trap into the OS
	. The OS will then notice that the page is a "copy-on-write"
	  page, and thus:
	  	. (lazily) allocate a new page,
	  	. fill it with the data,
	  	. and map this new page into the address space of the
	  	  faulting process
	. The process then continues and now has its own private copy
	  of the page


	. Useful because a shared library can be mapped COW into the
	  address space of many processes, saving valuable memory space

	. Useful in Unix systems due to the semantics of 'fork' and 'exec'
	. fork creates an exact copy of the address space of the caller
	. With a large address space, making such a copy is slow and
	  and uses a lot of space
	. Even worse, most of this address space is immediately
	  over-written by a subsequent call to exec, which overlays the
	  calling process's address space with that of the
	  soon-to-be-exec'd program
	. By instead performing a COW fork, the OS avoids much of the
	  needles copying

